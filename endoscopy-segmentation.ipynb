{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1574219,"sourceType":"datasetVersion","datasetId":930614}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## UNet Model\n\nimport torch\nimport torch.nn as nn\n\nimport torch\nimport torch.nn as nn\n\n\n\ndef double_convolution(in_channels, out_channels):\n    \"\"\"\n    In the original paper implementation, the convolution operations were\n    not padded but we are padding them here. This is because, we need the\n    output result size to be same as input size.\n    \"\"\"\n    conv_op = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels, affine=False, track_running_stats=False),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels, affine=False, track_running_stats=False),\n        nn.ReLU(inplace=True)\n    )\n    return conv_op\n\n\n\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n        # Contracting path.\n        # Each convolution is applied twice.\n        self.down_convolution_1 = double_convolution(3, 64)\n        self.down_convolution_2 = double_convolution(64, 128)\n        self.down_convolution_3 = double_convolution(128, 256)\n        self.down_convolution_4 = double_convolution(256, 512)\n        self.down_convolution_5 = double_convolution(512, 1024)\n\n        # Expanding path.\n        self.up_transpose_1 = nn.ConvTranspose2d(\n            in_channels=1024, out_channels=512,\n            kernel_size=2,\n            stride=2)\n        # Below, `in_channels` again becomes 1024 as we are concatinating.\n        self.up_convolution_1 = double_convolution(1024, 512)\n        self.up_transpose_2 = nn.ConvTranspose2d(\n            in_channels=512, out_channels=256,\n            kernel_size=2,\n            stride=2)\n        self.up_convolution_2 = double_convolution(512, 256)\n        self.up_transpose_3 = nn.ConvTranspose2d(\n            in_channels=256, out_channels=128,\n            kernel_size=2,\n            stride=2)\n        self.up_convolution_3 = double_convolution(256, 128)\n        self.up_transpose_4 = nn.ConvTranspose2d(\n            in_channels=128, out_channels=64,\n            kernel_size=2,\n            stride=2)\n        self.up_convolution_4 = double_convolution(128, 64)\n        # output => `out_channels` as per the number of classes.\n        self.out = nn.Conv2d(\n            in_channels=64, out_channels=3,\n            kernel_size=1\n        )\n\n    def forward(self, x):\n        # TODO: Write here!\n        down_1 = self.down_convolution_1(x)\n        down_2 = self.max_pool2d(down_1)\n        down_3 = self.down_convolution_2(down_2)\n        down_4 = self.max_pool2d(down_3)\n        down_5 = self.down_convolution_3(down_4)\n        down_6 = self.max_pool2d(down_5)\n        down_7 = self.down_convolution_4(down_6)\n        down_8 = self.max_pool2d(down_7)\n        down_9 = self.down_convolution_5(down_8)\n\n        up_1 = self.up_transpose_1(down_9)\n        up_2 = self.up_convolution_1(torch.cat([down_7, up_1], 1))\n        up_3 = self.up_transpose_2(up_2)\n        up_4 = self.up_convolution_2(torch.cat([down_5, up_3], 1))\n        up_5 = self.up_transpose_3(up_4)\n        up_6 = self.up_convolution_3(torch.cat([down_3, up_5], 1))\n        up_7 = self.up_transpose_4(up_6)\n        up_8 = self.up_convolution_4(torch.cat([down_1, up_7], 1))\n\n        out = self.out(up_8)\n\n        return out","metadata":{"_uuid":"1f905be6-e36e-4106-ac15-ce512b3a6c16","_cell_guid":"4274c4cf-28d7-48fc-a108-2df7879b1d60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms \nimport torchvision\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport time\nimport os\nfrom tqdm.notebook import tqdm\n\n\n# Define the device to be used for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\n## Define diceloss\nimport torch.nn.functional as F\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n## Load dataset\nimport os\nfrom glob import glob\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform_img=None, transform_msk=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform_img = transform_img\n        self.transform_msk = transform_msk\n        # Define the transformations to be applied to the images and masks\n        \n        \n        self.transform_img = transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0,0,0], std=[1,1,1])\n\n])\n\n        self.transform_msk = transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0,0,0], std=[1,1,1])\n\n])\n\n        self.image_paths = sorted(glob(os.path.join(self.image_dir, '*.png')))\n        self.mask_paths = sorted(glob(os.path.join(self.mask_dir, '*.png')))\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n\n        if not os.path.exists(img_path) or not os.path.exists(mask_path):\n            raise FileNotFoundError(f\"Image or mask file not found at index {idx}\")\n\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform_img(image)\n\n        mask = (Image.open(mask_path))\n        mask = self.transform_msk(mask)\n\n\n\n        return image, mask\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nfrom glob import glob\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch\nfrom torchvision import transforms\nimport cv2\nimport albumentations as album\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the paths to the training data\ntrain_image_dir = \"/kaggle/input/cvcclinicdb/PNG/Original\"\ntrain_mask_dir = \"/kaggle/input/cvcclinicdb/PNG/Ground Truth\"\n\n# Initialize the full dataset\nfull_dataset = CustomDataset(train_image_dir, train_mask_dir)\n\n# Define the percentages for splitting (e.g., 80% training, 10% validation, 10% testing)\ntrain_percentage = 0.8\nval_percentage = 0.1\ntest_percentage = 0.1\n\n# Ensure percentages sum to 1\nassert train_percentage + val_percentage + test_percentage == 1, \"Splitting percentages must sum to 1.\"\n\n# Calculate lengths for each split\ntrain_size = int(train_percentage * len(full_dataset))\nval_size = int(val_percentage * len(full_dataset))\ntest_size = len(full_dataset) - train_size - val_size  # Remaining samples for the test set\n\n# Split the dataset into training, validation, and testing sets\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n\n# Create DataLoaders for the training, validation, and testing datasets\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Define the training and testing datasets\ntrain_dataset = CustomDataset(train_image_dir, train_mask_dir)\ntest_dataset = CustomDataset(test_image_dir, test_mask_dir)\nval_dataset = CustomDataset(val_image_dir, val_mask_dir)\n\n# Define the training and testing data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)","metadata":{}},{"cell_type":"code","source":"\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n\n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)\n\n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n\n        intersection = (inputs * targets).sum()\n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n\n        return 1 - dice\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\n# Assuming UNet is defined and imported already\nmodel = UNet().to(device)\n\n# Setup loss functions and optimizer\nloss_fn_1 = DiceLoss()  # Replace with your actual Dice Loss class if needed\nloss_fn_2 = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n\n# Set the seed for reproducibility\ntorch.manual_seed(42)\n\n# Number of epochs for training\nepochs = 300\ntrain_total_losses = []\nval_total_losses = []\n\n# Training loop\nfor epoch in tqdm(range(epochs)):\n    train_losses, test_losses = [], []\n    print(f\"Epoch: {epoch+1} of {epochs}\")\n\n    ### Training\n    train_loss_1, train_loss_2, train_loss = 0, 0, 0\n    model.train()\n\n    for batch, (X, y) in enumerate(train_loader):\n        X, y = X.to(device), y.to(device)\n\n        # Forward pass\n        y_pred = model(X)\n\n        # Calculate loss\n        loss_1 = loss_fn_1(y_pred, y)\n        loss_2 = loss_fn_2(y_pred, y)\n        loss = loss_1 + loss_2\n        train_loss += loss.item()\n        train_loss_1 += loss_1.item()\n        train_loss_2 += loss_2.item()\n\n        # Zero gradients, backward pass, and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n    # Average training loss\n    train_loss /= len(train_loader)\n    train_loss_1 /= len(train_loader)\n    train_loss_2 /= len(train_loader)\n\n    ### Validation\n    test_loss_1, test_loss_2, test_loss = 0, 0, 0\n    model.eval()\n\n    for X, y in val_loader:\n        X, y = X.to(device), y.to(device)\n\n        # Forward pass\n        y_pred = model(X)\n\n        # Calculate loss\n        loss_1 = loss_fn_1(y_pred, y)\n        loss_2 = loss_fn_2(y_pred, y)\n        loss = loss_1 + loss_2\n        test_loss += loss.item()\n        test_loss_1 += loss_1.item()\n        test_loss_2 += loss_2.item()\n\n        test_losses.append(loss.item())\n\n    # Average validation loss\n    test_loss /= len(val_loader)\n    test_loss_1 /= len(val_loader)\n    test_loss_2 /= len(val_loader)\n\n    # Print out losses for training and validation\n    print(f\"Train loss: {train_loss:.5f}, Dice: {train_loss_1:.5f}, BCE: {train_loss_2:.5f} | Test loss: {test_loss:.5f}, Dice: {test_loss_1:.5f}, BCE: {test_loss_2:.5f}\\n\")\n\n    train_loss = np.average(train_losses)\n    train_total_losses.append(train_loss)\n    val_loss = np.average(test_losses)\n    val_total_losses.append(test_loss)\n\n    # Display predictions at the end of every 5 epochs\n    if epoch % 5 == 0:\n        # Display a batch of images and corresponding predicted masks\n        plt.figure(figsize=(15, 5))\n        plt.subplot(131)\n        plt.imshow(X[0, 0].cpu().detach().numpy(), cmap='gray')\n        plt.title(\"Input Image\")\n        plt.axis('off')\n\n        plt.subplot(132)\n        plt.imshow(y[0, 0].cpu().detach().numpy(), cmap='gray')\n        plt.title(\"Ground Truth Mask\")\n        plt.axis('off')\n\n        plt.subplot(133)\n        # Apply a threshold to the predicted mask for visualization\n        y_pred_mask = torch.sigmoid(y_pred[0, 0]).cpu().detach().numpy()  # Use sigmoid to convert logits to probabilities\n        plt.imshow(y_pred_mask, cmap='gray')\n        plt.title(\"Predicted Mask\")\n        plt.axis('off')\n\n        plt.show()\n\n    # Save the model and plot the losses every 5 epochs\n    if epoch % 5 == 0 and epoch != 0:\n        torch.save(model.state_dict(), f\"./model-{epoch}.pth\")\n        plt.figure(figsize=(20, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot(train_total_losses, label='train_loss')\n        plt.plot(val_total_losses, label='val_loss')\n        plt.title(\"Training & Validation Losses\")\n        plt.ylabel(\"Loss\")\n        plt.xlabel(\"Epochs\")\n        plt.legend()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Directory to save predicted masks if needed\noutput_dir = \"predicted_masks\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Iterate through the entire test dataset\nwith torch.no_grad():\n    for batch_idx, (images, gt_masks) in enumerate(test_loader):\n        # Move data to the device\n        images = images.to(device)\n        gt_masks = gt_masks.to(device)\n\n        # Forward pass\n        logits = model(images)\n        pr_masks = (logits.squeeze(1) > 0.5).float()\n\n        # Process each image in the batch\n        for i, (image, gt_mask, pr_mask) in enumerate(zip(images, gt_masks, pr_masks)):\n            # Convert tensors to NumPy arrays\n            image_np = image.permute(1, 2, 0).cpu().numpy()\n            gt_mask_np = gt_mask.squeeze().cpu().numpy()\n            pr_mask_np = pr_mask.cpu().numpy()\n\n            # Display the results\n            plt.figure(figsize=(15, 5))\n\n            plt.subplot(1, 3, 1)\n            plt.imshow(image_np)\n            plt.title(\"Image\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(gt_mask_np, cmap='gray')\n            plt.title(\"Ground Truth\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(pr_mask_np, cmap='gray')\n            plt.title(\"Prediction\")\n            plt.axis(\"off\")\n\n            plt.show()\n\n            # Optionally save the predicted mask\n            pr_mask_filename = os.path.join(output_dir, f\"pred_mask_batch{batch_idx}_img{i}.png\")\n            plt.imsave(pr_mask_filename, pr_mask_np, cmap='gray')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport os\nimport numpy as np\nfrom PIL import Image\n\n\nwith torch.inference_mode():\n    for X, y in test_loader:\n        #\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n\ny1 = y[0, 0].cpu().detach().numpy()\ny2 = y_pred[0, 0].cpu().detach().numpy()\n\nplt.subplot(121)\nplt.imshow(y1, cmap='gray')\n\nplt.subplot(122)\nplt.imshow(y2, cmap='gray')\nplt.show()\n\ny_color = np.zeros((*y2.shape, 3))\ny_color[..., 0] = y1\ny_color[..., 1] = y2\ny_color[..., 2] = y2\n\n\nplt.subplot(121)\nplt.imshow(X[0, 0].cpu().detach().numpy())\n\nplt.subplot(122)\nplt.imshow(y_color)\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming 'test_loader' is your DataLoader\nbatch = next(iter(test_loader))\n\nwith torch.no_grad():\n    model.eval()\n    logits = model(batch[0].to(device))\npr_masks = (logits.squeeze(1) > 0.5).float()\n\nfor image, gt_mask, pr_mask in zip(batch[0], batch[1], pr_masks):\n    plt.figure(figsize=(15, 5))  # Increase the width for better visualization\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(image.permute(1, 2, 0).cpu().numpy())  # Use permute for CHW to HWC conversion\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 3, 2)\n    # Convert RGB to grayscale for ground truth mask\n    grayscale_gt_mask = np.mean(gt_mask.squeeze().cpu().numpy(), axis=0)\n    plt.imshow(grayscale_gt_mask, cmap='gray')\n    plt.title(\"Ground truth\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    # Convert RGB to grayscale for prediction mask\n    if pr_mask.shape[0] == 3:\n        grayscale_pr_mask = np.mean(pr_mask.cpu().numpy(), axis=0)\n        plt.imshow(grayscale_pr_mask, cmap='gray')\n    else:\n        plt.imshow(pr_mask.cpu().numpy(), cmap='gray')\n    \n    plt.title(\"Prediction\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n\n\n\n# Assuming 'test_loader' is your DataLoader\nbatch = next(iter(test_loader))\n\nwith torch.no_grad():\n    model.eval()\n    logits = model(batch[0].to(device))  # Assuming image is at index 0\npr_masks = (logits.squeeze(1) > 0.5).float()\n\n# Iterate through the batches\nfor image, gt_mask, pr_mask in zip(batch[0], batch[1], pr_masks):\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 3, 2)\n    grayscale_gt_mask = np.mean(gt_mask.squeeze().cpu().numpy(), axis=0)\n    plt.imshow(grayscale_gt_mask, cmap='gray')\n    plt.title(\"Ground truth\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    if pr_mask.shape[0] == 3:\n        grayscale_pr_mask = np.mean(pr_mask.cpu().numpy(), axis=0)\n        plt.imshow(grayscale_pr_mask, cmap='gray')\n    else:\n        plt.imshow(pr_mask.cpu().numpy(), cmap='gray')\n    \n    plt.title(\"Prediction\")\n    plt.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}